{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization mit hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Activation\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_Bert():\n",
    "    news = pd.read_csv('../data/mixed_news/news_dataset.csv')\n",
    "    news = news.dropna(subset=['title','content'])\n",
    "    news = news[news.content != ' ']\n",
    "    news = news[news.title != ' ']\n",
    "    with h5py.File('encoded_data/title_encode.h5', 'r') as hf:\n",
    "        title_encode = hf['title_encode'][:]\n",
    "    title_NN = pd.DataFrame(data = title_encode)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(title_NN,news.label, test_size=0.33, stratify=news.label,\n",
    "                                                        random_state=42)\n",
    "    LE = LabelEncoder()\n",
    "    LE.fit([\"fake\",\"real\"])\n",
    "    y_train = LE.transform(y_train)\n",
    "    y_test = LE.transform(y_test)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_bow():\n",
    "    x_train = np.genfromtxt(\"../build/preprocessed/bow_X_train.txt\",unpack=True)\n",
    "    y_train = np.genfromtxt(\"../build/preprocessed/bow_y_train.txt\", unpack=True)\n",
    "    x_test  = np.genfromtxt(\"../build/preprocessed/bow_X_test.txt\",unpack=True)\n",
    "    y_test  = np.genfromtxt(\"../build/preprocessed/bow_y_test.txt\", unpack=True)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.genfromtxt(\"../build/preprocessed/bow_X_train.txt\",unpack=True)\n",
    "y_train = np.genfromtxt(\"../build/preprocessed/bow_y_train.txt\", unpack=True)\n",
    "x_test  = np.genfromtxt(\"../build/preprocessed/bow_X_test.txt\",unpack=True)\n",
    "y_test  = np.genfromtxt(\"../build/preprocessed/bow_y_test.txt\", unpack=True)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int({{normal(1000, 200)}}), input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    if {{choice(['three', 'four'])}} == 'four':\n",
    "        model.add(Dense(int({{normal( 1000, 200)}})))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "    model.add(Dense({{randint(300)}}))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer='adam')\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_split=0.3,\n",
    "              callbacks=[early_stop])\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss={{choice(['hinge','binary_crossentropy','squared_hinge'])}}, metrics=['accuracy'],\n",
    "                  optimizer={{choice(['adam','AdaDelta','Adagrad'])}})\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_split=0.3,\n",
    "              callbacks=[early_stop])\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_regularization(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, kernel_regularizer=l1({{uniform(0,0.1)}}), input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1000),kernel_regularizer=l2({{uniform(0,0.1)}}))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10),kernel_regularizer=l2({{uniform(0,0.1)}}))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer='adam')\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_split=0.3,\n",
    "              callbacks=[early_stop])\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with hyperopt\n",
    "Algorithm: Tree of Parzen Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import h5py\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import roc_curve, auc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, LeakyReLU, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import TensorBoard, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix, classification_report\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, normal\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'int': hp.normal('int', 1000, 200),\n",
      "        'int_1': hp.choice('int_1', ['three', 'four']),\n",
      "        'int_2': hp.normal('int_2',  1000, 200),\n",
      "        'Dense': hp.randint('Dense', 300),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: x_train = np.genfromtxt(\"../build/preprocessed/bow_X_train.txt\",unpack=True)\n",
      "  3: y_train = np.genfromtxt(\"../build/preprocessed/bow_y_train.txt\", unpack=True)\n",
      "  4: x_test  = np.genfromtxt(\"../build/preprocessed/bow_X_test.txt\",unpack=True)\n",
      "  5: y_test  = np.genfromtxt(\"../build/preprocessed/bow_y_test.txt\", unpack=True)\n",
      "  6: \n",
      "  7: \n",
      "  8: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     dim = x_train.shape[1]\n",
      "  4:     model = Sequential()\n",
      "  5:     model.add(Dense(int(space['int']), input_dim=dim))\n",
      "  6:     model.add(Activation('relu'))\n",
      "  7:     \n",
      "  8:     if space['int_1'] == 'four':\n",
      "  9:         model.add(Dense(int(space['int_2'])))\n",
      " 10:         model.add(Activation('relu'))\n",
      " 11:         \n",
      " 12:     model.add(Dense(space['Dense']))\n",
      " 13:     model.add(Activation('relu'))\n",
      " 14:     model.add(Dense(1))\n",
      " 15:     model.add(Activation('sigmoid'))\n",
      " 16: \n",
      " 17:     model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
      " 18:                   optimizer='adam')\n",
      " 19: \n",
      " 20:     early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
      " 21: \n",
      " 22:     result = model.fit(x_train, y_train,\n",
      " 23:               batch_size=64,\n",
      " 24:               epochs=30,\n",
      " 25:               verbose=2,\n",
      " 26:               validation_split=0.3,\n",
      " 27:               callbacks=[early_stop])\n",
      " 28:     validation_acc = np.amax(result.history['val_acc']) \n",
      " 29:     print('Best validation acc of epoch:', validation_acc)\n",
      " 30:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      " 31: \n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model_structure,\n",
    "                                      data=data_bow,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=trials,\n",
    "                                     notebook_name='Hyperopt')\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.save('../model/best_Hyperopt_NN_Bert.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()\n",
    "best_model = load_model('../model/best_Hyperopt_NN_Bert.hdf5')\n",
    "y_pred = best_model.predict(X_test, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.round(y_pred)\n",
    "print(classification_report(Y_test, y_pred_bool))\n",
    "print(confusion_matrix(Y_test, y_pred_bool,\n",
    "                       labels=[0,1]))\n",
    "plt.imshow(confusion_matrix(Y_test, y_pred_bool,\n",
    "                            labels=[0,1]))\n",
    "plt.tight_layout()\n",
    "plt.colorbar()\n",
    "plt.xticks(range(2), [\"fake\", \"real\"])\n",
    "plt.yticks(range(2), [\"fake\", \"real\"])\n",
    "plt.show()\n",
    "#plt.savefig(\"../build/plots/Bert/cnfsn_mtx_Hyperopt_bert_title_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "plt.hist(y_pred[Y_test == 0],label=\"fake\", alpha = 0.4, color = \"r\")\n",
    "plt.hist(y_pred[Y_test == 1],label = \"real\",alpha = 0.4, color = \"b\")\n",
    "plt.xlabel(\"Prediction Probability\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.savefig(\"../build/plots/Bert/prob_Hyperopt_bert_title_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fpr, tpr, _ = roc_curve(Y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "#plt.savefig(\"../build/plots/Bert/roc_Hyperopt_bert_title_nn.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.get_config()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
