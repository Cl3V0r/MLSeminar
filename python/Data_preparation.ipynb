{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/larsmoellerherm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/larsmoellerherm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from langdetect import detect\n",
    "import progressbar\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "tknzr = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting to know the Dataset\n",
    "\n",
    "Es gibt einige Texte die nicht auf Englisch sind und ein paar, die nur leerzeichen, nans,... enthalten.\n",
    "Diese werden erst gelöscht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (28665 of 28665) |##################| Elapsed Time: 0:11:27 Time:  0:11:27\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_csv('../data/mixed_news/news_dataset.csv')\n",
    "news = news.drop([\"title\",\"Unnamed: 0\",\"publication\"],axis=1)\n",
    "news = news.dropna()\n",
    "\n",
    "wrong_indexes = []\n",
    "counter = 0\n",
    "for text in progressbar.progressbar(news.content):\n",
    "    try:\n",
    "        if detect(text) != 'en':\n",
    "            wrong_indexes.append(counter)\n",
    "    except:\n",
    "        wrong_indexes.append(counter)\n",
    "    counter +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.drop(wrong_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label\n",
       "0  Print They should pay all the back all the mon...  fake\n",
       "1  Why Did Attorney General Loretta Lynch Plead T...  fake\n",
       "2  Red State : \\nFox News Sunday reported this mo...  fake\n",
       "3  Email Kayla Mueller was a prisoner and torture...  fake\n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  fake"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News 0.437 prozent\n",
      "Real News 0.563 prozent\n",
      "Gesamtgröße des Datasets: 27903\n"
     ]
    }
   ],
   "source": [
    "print(\"Fake News %.3f prozent\" % (news[news.label=='fake'].shape[0]/news.shape[0]))\n",
    "print(\"Real News %.3f prozent\" % (news[news.label=='real'].shape[0]/news.shape[0]))\n",
    "print(\"Gesamtgröße des Datasets: %i\" % news.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations=\"?:!.,;/\"\n",
    "class Splitter(object):\n",
    "    \"\"\"\n",
    "    split the document into sentences and tokenize each sentence\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "        \"\"\"\n",
    "        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n",
    "        \"\"\"\n",
    "        # split into single sentence\n",
    "        sentences = self.splitter.tokenize(text)\n",
    "        # tokenization in each sentences\n",
    "        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokens\n",
    "    \n",
    "spl = Splitter()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "def lemmatizing(s):\n",
    "    tokens = spl.split(s)\n",
    "    pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "    pos_t = [[lemmatizer.lemmatize(word,get_wordnet_pos(pos_tag)) for (word,pos_tag) in pos if word not in punctuations] for pos in pos_tokens]\n",
    "    sentences = [\" \".join(x) for x in pos_t]\n",
    "    return \" \".join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['lem_content'] = news['content'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red State Fox News Sunday report this morning that Anthony Weiner be cooperate with the FBI which have re-opened ( yes lefty “ re-opened ” ) the investigation into Hillary Clinton ’ s classify email Watch a Chris Wallace report the break news during the panel segment near the end of the show And the news be break while we ’ re on the air Our colleague Bret Baier have just send u an e-mail saying he have two source who say that Anthony Weiner who also have co-ownership of that laptop with his estranged wife Huma Abedin be cooperate with the FBI investigation have give them the laptop so therefore they didn ’ t need a warrant to get in to see the content of say laptop Pretty interesting development Targets of federal investigation will often cooperate hop that they will get consideration from a judge at sentence Given Weiner ’ s well-known penchant for lie it ’ s hard to believe that a prosecutor would give Weiner a deal base on an agreement to testify unless his testimony be very strongly corroborate by hard evidence But cooperation can take many form — and a Wallace indicate on this morning ’ s show one of those form could be sign a consent form to allow the content of device that they could probably get a warrant for anyway We ’ ll see if Weiner ’ s cooperation extend beyond that More Related\n",
      "Red State : \n",
      "Fox News Sunday reported this morning that Anthony Weiner is cooperating with the FBI, which has re-opened (yes, lefties: “re-opened”) the investigation into Hillary Clinton’s classified emails. Watch as Chris Wallace reports the breaking news during the panel segment near the end of the show: \n",
      "And the news is breaking while we’re on the air. Our colleague Bret Baier has just sent us an e-mail saying he has two sources who say that Anthony Weiner, who also had co-ownership of that laptop with his estranged wife Huma Abedin, is cooperating with the FBI investigation, had given them the laptop, so therefore they didn’t need a warrant to get in to see the contents of said laptop. Pretty interesting development. \n",
      "Targets of federal investigations will often cooperate, hoping that they will get consideration from a judge at sentencing. Given Weiner’s well-known penchant for lying, it’s hard to believe that a prosecutor would give Weiner a deal based on an agreement to testify, unless his testimony were very strongly corroborated by hard evidence. But cooperation can take many forms — and, as Wallace indicated on this morning’s show, one of those forms could be signing a consent form to allow   the contents of devices that they could probably get a warrant for anyway. We’ll see if Weiner’s cooperation extends beyond that. More Related\n"
     ]
    }
   ],
   "source": [
    "print(news.lem_content[2])\n",
    "print(news.content[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.to_hdf(\"../build/preprocessed/lemmatized_news.hdf5\",key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "take 500 most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>lem_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Red State Fox News Sunday report this morning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Email Kayla Mueller be a prisoner and torture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label  \\\n",
       "0  Print They should pay all the back all the mon...  fake   \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...  fake   \n",
       "2  Red State : \\nFox News Sunday reported this mo...  fake   \n",
       "3  Email Kayla Mueller was a prisoner and torture...  fake   \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  fake   \n",
       "\n",
       "                                         lem_content  \n",
       "0  Print They should pay all the back all the mon...  \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...  \n",
       "2  Red State Fox News Sunday report this morning ...  \n",
       "3  Email Kayla Mueller be a prisoner and torture ...  \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 500\n",
    "news = pd.read_hdf(\"../build/preprocessed/lemmatized_news.hdf5\",key=\"data\")\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(news.lem_content,  news.label, test_size=0.3, random_state=seed, shuffle=True, stratify=news.label)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=dim, ngram_range=(1,1))\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "used_words = vectorizer.get_feature_names()\n",
    "\n",
    "X_train_bow = vectorizer.transform(X_train).toarray()\n",
    "X_test_bow = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = LabelEncoder()\n",
    "LE.fit([\"fake\",\"real\"])\n",
    "y_train_enc = LE.transform(y_train)\n",
    "y_test_enc = LE.transform(y_test)\n",
    "\n",
    "x_train = pd.DataFrame(data=X_train_bow,columns=used_words)\n",
    "x_train[\"label\"] = y_train_enc\n",
    "x_test = pd.DataFrame(data=X_test_bow,columns=used_words)\n",
    "x_test[\"label\"] = y_test_enc\n",
    "\n",
    "x_train.to_hdf(\"../build/preprocessed/bow_data.hdf5\",key=\"train\")\n",
    "x_test.to_hdf(\"../build/preprocessed/bow_data.hdf5\",key=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
