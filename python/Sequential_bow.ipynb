{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimierung und Evaluierung des Sequential bow Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sn\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Activation\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, normal, qlognormal, randint\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are the Hyperparameter distributed\n",
    "\n",
    "### Strukture\n",
    "\n",
    "size of first and second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.round(np.random.lognormal(7,0.5,10000)/10)*10\n",
    "plt.hist(x,bins=100)\n",
    "plt.xlim(0,5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size of third hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.round(np.random.lognormal(4,0.5,10000)/1)*1\n",
    "plt.hist(x,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =np.random.uniform(0,0.1,10000)\n",
    "plt.hist(x,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_bow():\n",
    "    x_test = np.genfromtxt(\"../build/preprocessed/bow_X_test.txt\")\n",
    "    x_train = np.genfromtxt(\"../build/preprocessed/bow_X_train.txt\")\n",
    "    y_test = np.genfromtxt(\"../build/preprocessed/bow_y_test.txt\")\n",
    "    y_train = np.genfromtxt(\"../build/preprocessed/bow_y_train.txt\")\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_structure(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense({{qlognormal(7,0.5,10)}}, input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    if {{choice(['three', 'four'])}} == 'four':\n",
    "        model.add(Dense({{qlognormal(7,0.5,10)}}))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "    model.add(Dense({{qlognormal(4,0.5,1)}}))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer='adam')\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_split=0.3)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(np.abs(1187.5872913047178)), input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(int(np.abs(-1475.2916969518506))))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(261))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss={{choice(['hinge','binary_crossentropy','squared_hinge'])}}, metrics=['accuracy'],\n",
    "                  optimizer={{choice(['adam','AdaDelta','Adagrad'])}})\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_split=0.3)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_regularization(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(np.abs(1187.5872913047178)), kernel_regularizer=l1({{uniform(0,0.1)}}), input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(int(np.abs(-1475.2916969518506)),kernel_regularizer=l2({{uniform(0,0.1)}})))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(261,kernel_regularizer=l2({{uniform(0,0.1)}})))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer='Adagrad')\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_split=0.3)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with hyperopt\n",
    "Algorithm: Tree of Parzen Estimators\n",
    "Optimierung in 3 Schritten:\n",
    "    - Struktur (Tiefe (2 oder 3 hidden Layers) und Breite)\n",
    "    - Training (loss function und optimizer)\n",
    "    - Regularizierung ( L1 für die erste Layer und L2 für 2 und 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model_structure,\n",
    "                                      data=data_bow,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=trials,\n",
    "                                     notebook_name='Hyperopt')\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.save('../model/best_Hyperopt_NN_bow_struct.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model_training,\n",
    "                                      data=data_bow,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=15,\n",
    "                                      trials=trials,\n",
    "                                     notebook_name='Hyperopt')\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.save('../model/best_Hyperopt_NN_bow_training.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model_regularization,\n",
    "                                      data=data_bow,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=80,\n",
    "                                      trials=trials,\n",
    "                                     notebook_name='Hyperopt')\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.save('../model/best_Hyperopt_NN_bow_regularization.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train best model\n",
    "Neues Training des besten Modells, welches Optimiert bezüglich der Hyperparameter ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.savefig(\"../build/plots/bow/history_bow_best.pdf\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_281 (Dense)            (None, 1187)              1188187   \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 1187)              0         \n",
      "_________________________________________________________________\n",
      "dense_282 (Dense)            (None, 1475)              1752300   \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 1475)              0         \n",
      "_________________________________________________________________\n",
      "dense_283 (Dense)            (None, 261)               385236    \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 261)               0         \n",
      "_________________________________________________________________\n",
      "dense_284 (Dense)            (None, 1)                 262       \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,325,985\n",
      "Trainable params: 3,325,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data_bow()\n",
    "best_model = load_model('../model/best_Hyperopt_NN_bow_regularization.hdf5')\n",
    "model = Sequential.from_config(best_model.get_config())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13659 samples, validate on 5854 samples\n",
      "Epoch 1/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 2.6207 - acc: 0.8371 - val_loss: 0.4891 - val_acc: 0.8982\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48909, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 2/100\n",
      "13659/13659 [==============================] - 18s 1ms/step - loss: 0.4392 - acc: 0.9031 - val_loss: 0.4911 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.48909\n",
      "Epoch 3/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.3796 - acc: 0.9143 - val_loss: 0.3880 - val_acc: 0.9042\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48909 to 0.38804, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 4/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.3331 - acc: 0.9261 - val_loss: 0.3799 - val_acc: 0.9011\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38804 to 0.37990, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 5/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.3035 - acc: 0.9359 - val_loss: 0.3641 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.37990 to 0.36405, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 6/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.2842 - acc: 0.9432 - val_loss: 0.3497 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36405 to 0.34973, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 7/100\n",
      "13659/13659 [==============================] - 21s 2ms/step - loss: 0.2669 - acc: 0.9492 - val_loss: 0.3531 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.34973\n",
      "Epoch 8/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.2525 - acc: 0.9560 - val_loss: 0.3565 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.34973\n",
      "Epoch 9/100\n",
      "13659/13659 [==============================] - 21s 2ms/step - loss: 0.2383 - acc: 0.9626 - val_loss: 0.3459 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.34973 to 0.34595, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 10/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.2267 - acc: 0.9668 - val_loss: 0.3482 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34595\n",
      "Epoch 11/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.2153 - acc: 0.9730 - val_loss: 0.3594 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34595\n",
      "Epoch 12/100\n",
      "13659/13659 [==============================] - 22s 2ms/step - loss: 0.2150 - acc: 0.9724 - val_loss: 0.3509 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34595\n",
      "Epoch 13/100\n",
      "13659/13659 [==============================] - 22s 2ms/step - loss: 0.2014 - acc: 0.9781 - val_loss: 0.3521 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.34595\n",
      "Epoch 14/100\n",
      "13659/13659 [==============================] - 22s 2ms/step - loss: 0.1914 - acc: 0.9813 - val_loss: 0.3498 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.34595\n",
      "Epoch 15/100\n",
      "13659/13659 [==============================] - 23s 2ms/step - loss: 0.1841 - acc: 0.9854 - val_loss: 0.3549 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.34595\n",
      "Epoch 16/100\n",
      "13659/13659 [==============================] - 24s 2ms/step - loss: 0.1793 - acc: 0.9873 - val_loss: 0.3509 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34595\n",
      "Epoch 17/100\n",
      "13659/13659 [==============================] - 24s 2ms/step - loss: 0.1739 - acc: 0.9889 - val_loss: 0.3542 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.34595\n",
      "Epoch 18/100\n",
      "13659/13659 [==============================] - 24s 2ms/step - loss: 0.1694 - acc: 0.9896 - val_loss: 0.3419 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.34595 to 0.34193, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 19/100\n",
      "13659/13659 [==============================] - 24s 2ms/step - loss: 0.1660 - acc: 0.9906 - val_loss: 0.3459 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.34193\n",
      "Epoch 20/100\n",
      "13659/13659 [==============================] - 25s 2ms/step - loss: 0.1632 - acc: 0.9912 - val_loss: 0.3469 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.34193\n",
      "Epoch 21/100\n",
      "13659/13659 [==============================] - 28s 2ms/step - loss: 0.1593 - acc: 0.9922 - val_loss: 0.3470 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.34193\n",
      "Epoch 22/100\n",
      "13659/13659 [==============================] - 27s 2ms/step - loss: 0.1564 - acc: 0.9922 - val_loss: 0.3506 - val_acc: 0.9148\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.34193\n",
      "Epoch 23/100\n",
      "13659/13659 [==============================] - 25s 2ms/step - loss: 0.1533 - acc: 0.9924 - val_loss: 0.3502 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.34193\n",
      "Epoch 24/100\n",
      "13659/13659 [==============================] - 24s 2ms/step - loss: 0.1511 - acc: 0.9932 - val_loss: 0.3467 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.34193\n",
      "Epoch 25/100\n",
      "13659/13659 [==============================] - 23s 2ms/step - loss: 0.1488 - acc: 0.9934 - val_loss: 0.3499 - val_acc: 0.9129\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.34193\n",
      "Epoch 26/100\n",
      "13659/13659 [==============================] - 22s 2ms/step - loss: 0.1465 - acc: 0.9941 - val_loss: 0.3457 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.34193\n",
      "Epoch 27/100\n",
      "13659/13659 [==============================] - 21s 2ms/step - loss: 0.1445 - acc: 0.9942 - val_loss: 0.3453 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.34193\n",
      "Epoch 28/100\n",
      "13659/13659 [==============================] - 23s 2ms/step - loss: 0.1426 - acc: 0.9944 - val_loss: 0.3483 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.34193\n",
      "Epoch 29/100\n",
      "13659/13659 [==============================] - 24s 2ms/step - loss: 0.1409 - acc: 0.9941 - val_loss: 0.3451 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.34193\n",
      "Epoch 30/100\n",
      "13659/13659 [==============================] - 26s 2ms/step - loss: 0.1396 - acc: 0.9950 - val_loss: 0.3418 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.34193 to 0.34175, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 31/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1378 - acc: 0.9951 - val_loss: 0.3421 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.34175\n",
      "Epoch 32/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1364 - acc: 0.9951 - val_loss: 0.3427 - val_acc: 0.9161\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.34175\n",
      "Epoch 33/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1349 - acc: 0.9955 - val_loss: 0.3468 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.34175\n",
      "Epoch 34/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1337 - acc: 0.9958 - val_loss: 0.3434 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.34175\n",
      "Epoch 35/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1325 - acc: 0.9956 - val_loss: 0.3432 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.34175\n",
      "Epoch 36/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1312 - acc: 0.9958 - val_loss: 0.3428 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.34175\n",
      "Epoch 37/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1299 - acc: 0.9957 - val_loss: 0.3502 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.34175\n",
      "Epoch 38/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1291 - acc: 0.9963 - val_loss: 0.3467 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.34175\n",
      "Epoch 39/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1281 - acc: 0.9963 - val_loss: 0.3454 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.34175\n",
      "Epoch 40/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1271 - acc: 0.9963 - val_loss: 0.3456 - val_acc: 0.9168\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.34175\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13659/13659 [==============================] - 18s 1ms/step - loss: 0.1263 - acc: 0.9966 - val_loss: 0.3436 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.34175\n",
      "Epoch 42/100\n",
      "13659/13659 [==============================] - 18s 1ms/step - loss: 0.1252 - acc: 0.9963 - val_loss: 0.3402 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.34175 to 0.34018, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 43/100\n",
      "13659/13659 [==============================] - 18s 1ms/step - loss: 0.1244 - acc: 0.9967 - val_loss: 0.3440 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.34018\n",
      "Epoch 44/100\n",
      "13659/13659 [==============================] - 18s 1ms/step - loss: 0.1236 - acc: 0.9966 - val_loss: 0.3431 - val_acc: 0.9161\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.34018\n",
      "Epoch 45/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1227 - acc: 0.9969 - val_loss: 0.3462 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.34018\n",
      "Epoch 46/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1220 - acc: 0.9968 - val_loss: 0.3430 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.34018\n",
      "Epoch 47/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1211 - acc: 0.9968 - val_loss: 0.3404 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.34018\n",
      "Epoch 48/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1203 - acc: 0.9969 - val_loss: 0.3385 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.34018 to 0.33850, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 49/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1196 - acc: 0.9971 - val_loss: 0.3424 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.33850\n",
      "Epoch 50/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1187 - acc: 0.9974 - val_loss: 0.3427 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.33850\n",
      "Epoch 51/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1181 - acc: 0.9971 - val_loss: 0.3432 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.33850\n",
      "Epoch 52/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1176 - acc: 0.9972 - val_loss: 0.3395 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.33850\n",
      "Epoch 53/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1169 - acc: 0.9972 - val_loss: 0.3407 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.33850\n",
      "Epoch 54/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1163 - acc: 0.9971 - val_loss: 0.3382 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.33850 to 0.33821, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 55/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1158 - acc: 0.9972 - val_loss: 0.3413 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.33821\n",
      "Epoch 56/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1151 - acc: 0.9975 - val_loss: 0.3359 - val_acc: 0.9173\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.33821 to 0.33586, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 57/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1145 - acc: 0.9975 - val_loss: 0.3428 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.33586\n",
      "Epoch 58/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1139 - acc: 0.9974 - val_loss: 0.3378 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.33586\n",
      "Epoch 59/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1134 - acc: 0.9976 - val_loss: 0.3408 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.33586\n",
      "Epoch 60/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1128 - acc: 0.9977 - val_loss: 0.3428 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.33586\n",
      "Epoch 61/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1123 - acc: 0.9979 - val_loss: 0.3395 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.33586\n",
      "Epoch 62/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1118 - acc: 0.9977 - val_loss: 0.3368 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.33586\n",
      "Epoch 63/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1113 - acc: 0.9979 - val_loss: 0.3392 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.33586\n",
      "Epoch 64/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1108 - acc: 0.9977 - val_loss: 0.3376 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.33586\n",
      "Epoch 65/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1103 - acc: 0.9980 - val_loss: 0.3373 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.33586\n",
      "Epoch 66/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1098 - acc: 0.9981 - val_loss: 0.3376 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.33586\n",
      "Epoch 67/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1094 - acc: 0.9980 - val_loss: 0.3369 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.33586\n",
      "Epoch 68/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1088 - acc: 0.9982 - val_loss: 0.3374 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.33586\n",
      "Epoch 69/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1084 - acc: 0.9978 - val_loss: 0.3387 - val_acc: 0.9168\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.33586\n",
      "Epoch 70/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1080 - acc: 0.9980 - val_loss: 0.3392 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.33586\n",
      "Epoch 71/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1075 - acc: 0.9980 - val_loss: 0.3401 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.33586\n",
      "Epoch 72/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1071 - acc: 0.9982 - val_loss: 0.3403 - val_acc: 0.9144\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.33586\n",
      "Epoch 73/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1066 - acc: 0.9982 - val_loss: 0.3378 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.33586\n",
      "Epoch 74/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1062 - acc: 0.9983 - val_loss: 0.3450 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.33586\n",
      "Epoch 75/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1059 - acc: 0.9982 - val_loss: 0.3372 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.33586\n",
      "Epoch 76/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1054 - acc: 0.9980 - val_loss: 0.3364 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.33586\n",
      "Epoch 77/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1050 - acc: 0.9982 - val_loss: 0.3335 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.33586 to 0.33351, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 78/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1047 - acc: 0.9982 - val_loss: 0.3366 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.33351\n",
      "Epoch 79/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1043 - acc: 0.9981 - val_loss: 0.3373 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.33351\n",
      "Epoch 80/100\n",
      "13659/13659 [==============================] - 20s 1ms/step - loss: 0.1039 - acc: 0.9983 - val_loss: 0.3394 - val_acc: 0.9144\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.33351\n",
      "Epoch 81/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1034 - acc: 0.9982 - val_loss: 0.3371 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.33351\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1032 - acc: 0.9983 - val_loss: 0.3370 - val_acc: 0.9168\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.33351\n",
      "Epoch 83/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1028 - acc: 0.9982 - val_loss: 0.3370 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.33351\n",
      "Epoch 84/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1024 - acc: 0.9983 - val_loss: 0.3348 - val_acc: 0.9142\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.33351\n",
      "Epoch 85/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1021 - acc: 0.9982 - val_loss: 0.3341 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.33351\n",
      "Epoch 86/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1018 - acc: 0.9982 - val_loss: 0.3349 - val_acc: 0.9148\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.33351\n",
      "Epoch 87/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1014 - acc: 0.9983 - val_loss: 0.3336 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.33351\n",
      "Epoch 88/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1011 - acc: 0.9984 - val_loss: 0.3326 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.33351 to 0.33260, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 89/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1007 - acc: 0.9983 - val_loss: 0.3386 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.33260\n",
      "Epoch 90/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1005 - acc: 0.9983 - val_loss: 0.3335 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.33260\n",
      "Epoch 91/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.1001 - acc: 0.9983 - val_loss: 0.3350 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.33260\n",
      "Epoch 92/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0998 - acc: 0.9983 - val_loss: 0.3343 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.33260\n",
      "Epoch 93/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0996 - acc: 0.9984 - val_loss: 0.3336 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.33260\n",
      "Epoch 94/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0992 - acc: 0.9983 - val_loss: 0.3314 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.33260 to 0.33141, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 95/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0989 - acc: 0.9984 - val_loss: 0.3393 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.33141\n",
      "Epoch 96/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0986 - acc: 0.9984 - val_loss: 0.3295 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.33141 to 0.32954, saving model to ../model/best_Hyperopt_NN_bow_trained.hdf5\n",
      "Epoch 97/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0983 - acc: 0.9985 - val_loss: 0.3420 - val_acc: 0.9107\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.32954\n",
      "Epoch 98/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0981 - acc: 0.9984 - val_loss: 0.3369 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.32954\n",
      "Epoch 99/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0977 - acc: 0.9983 - val_loss: 0.3445 - val_acc: 0.9148\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.32954\n",
      "Epoch 100/100\n",
      "13659/13659 [==============================] - 19s 1ms/step - loss: 0.0975 - acc: 0.9985 - val_loss: 0.3334 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.32954\n"
     ]
    }
   ],
   "source": [
    "filepath = '../model/best_Hyperopt_NN_bow_trained.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adagrad',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, validation_split=0.3,\n",
    "                    epochs=100,batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best model\n",
    "Betrachten des trainierten Modells. Darstellung der Confusion Matrix, Overtraining Plot und ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8364/8364 [==============================] - 2s 220us/step\n",
      "19513/19513 [==============================] - 5s 246us/step\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model('../model/best_Hyperopt_NN_bow_trained.hdf5')\n",
    "y_pred = best_model.predict(X_test, batch_size=64, verbose=1)\n",
    "y_pred_train = best_model.predict(X_train, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.91      0.90      3650\n",
      "         1.0       0.93      0.92      0.92      4714\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8364\n",
      "   macro avg       0.91      0.91      0.91      8364\n",
      "weighted avg       0.92      0.92      0.92      8364\n",
      "\n",
      "[[3324  326]\n",
      " [ 381 4333]]\n",
      "Predicted   0.0   1.0\n",
      "Actual               \n",
      "0.0        3324   326\n",
      "1.0         381  4333\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, y_pred_bool))\n",
    "print(confusion_matrix(Y_test, y_pred_bool,labels=[0,1]))\n",
    "\n",
    "#Confusion Matrix\n",
    "cnfn_matrix = pd.crosstab(Y_test, y_pred_bool[:,0], rownames=['Actual'], colnames=['Predicted'])\n",
    "print(cnfn_matrix)\n",
    "cnfn_matrix.columns = ['fake','real']\n",
    "cnfn_matrix = cnfn_matrix.rename_axis(\"Predicted\", axis=\"columns\")\n",
    "cnfn_matrix.rename(index = {0.0: \"fake\", 1.0:'real'}, inplace = True) \n",
    "cnfn_matrix = cnfn_matrix/Y_test.shape[0]\n",
    "sn.heatmap(cnfn_matrix, annot=True , cmap='viridis')\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/cnfsn_mtx_bow_best_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "#Overtraining test\n",
    "plt.hist(y_pred[Y_test == 0],label=\"fake test\", alpha = 0.4, color = \"r\",density=True)\n",
    "plt.hist(y_pred_train[Y_train == 0],label='fake train', alpha = 0.4, color = 'r', histtype='step',density=True)\n",
    "plt.hist(y_pred[Y_test == 1],label = \"real test\",alpha = 0.4, color = \"b\",density=True)\n",
    "plt.hist(y_pred_train[Y_train == 1],label='real train', alpha = 0.4, color = 'b', histtype='step',density=True)\n",
    "\n",
    "plt.xlabel(\"Prediction Probability\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.legend(loc='upper center')\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/prob_bow_best_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fpr, tpr, _ = roc_curve(Y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/roc_Hyperopt_bow_best_nn.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### Wordclkoud confusion matrix\n",
    "\n",
    "Darstellung der Wordhäufigkeiten in WordClouds für FP,FN,TP,TN getrennt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'prediction':y_pred[:,0],'prediction_bool':y_pred_bool[:,0],'truth':Y_test})\n",
    "results['dist'] = np.abs(results.truth - results.prediction)\n",
    "results = results.sort_values('dist',axis=0,ascending=False)\n",
    "FP = results[(results.prediction_bool== 1) & (results.truth == 0)]\n",
    "FN = results[(results.prediction_bool== 0) & (results.truth == 1)]\n",
    "TP = results[(results.prediction_bool== 1) & (results.truth == 1)]\n",
    "TN = results[(results.prediction_bool== 0) & (results.truth == 0)]\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_FP = X_test.loc[FP.index]\n",
    "X_FN = X_test.loc[FN.index]\n",
    "X_TP = X_test.loc[TP.index]\n",
    "X_TN = X_test.loc[TN.index]\n",
    "f = open(\"../build/preprocessed/bow_feature_names.txt\", \"r\")\n",
    "words = [x[:-1] for x in f]\n",
    "X_FP.columns = words\n",
    "X_FN.columns = words\n",
    "X_TP.columns = words\n",
    "X_TN.columns = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "def plotWordcloud_cnfn(TN,FN,FP,TP):    \n",
    "    TN = TN.sum().to_dict()\n",
    "    FN = FN.sum().to_dict()\n",
    "    FP = FP.sum().to_dict()\n",
    "    TP = TP.sum().to_dict()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10),dpi=100)\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                          width=1920,\n",
    "                          height=1080,\n",
    "                          mask=np.array(Image.open('../data/pictures/trump_silhouette.png'))\n",
    "                          ).generate_from_frequencies(TN)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                          width=1920,\n",
    "                          height=1080,\n",
    "                          mask= np.array(Image.open('../data/pictures/trump_silhouette.png'))\n",
    "                          ).generate_from_frequencies(FP)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                          width=1920,\n",
    "                          height=1080,\n",
    "                          mask=np.array(Image.open('../data/pictures/USA.jpg'))\n",
    "                          ).generate_from_frequencies(FN)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                          width=1920,\n",
    "                          height=1080,\n",
    "                          mask=np.array(Image.open('../data/pictures/USA.jpg'))\n",
    "                          ).generate_from_frequencies(TP)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    #plt.show()\n",
    "    plt.savefig(\"../build/plots/bow/cnfn_wordcloud.pdf\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotWordcloud_cnfn(X_TN,X_FN,X_FP,X_TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud fake real news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWordcloud(content,t):\n",
    "    if(t!=\"\"):\n",
    "       mask = np.array(Image.open('../data/pictures/'+t))\n",
    "    else:\n",
    "        mask=None\n",
    "        \n",
    "\n",
    "    content = content.sum().to_dict()\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                      width=1920,\n",
    "                      height=1080,\n",
    "                      mask=mask\n",
    "                      ).generate_from_frequencies(content)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(X_test,X_train,axis=0)\n",
    "y = np.append(Y_test,Y_train,axis=0)\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = words\n",
    "plotWordcloud(X[y==0],\"trump_silhouette.png\")\n",
    "plt.savefig(\"../build/plots/fake_wordcloud.pdf\")\n",
    "plt.close()\n",
    "plotWordcloud(X[y==1],\"USA.jpg\")\n",
    "plt.savefig(\"../build/plots/real_wordcloud.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untersuchung der first layer\n",
    "\n",
    "Summieren der Beträge aller Gewichte eines Neurons ohne Offset und anschließende Darstellung in WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_weights = best_model.layers[0].get_weights()[0]\n",
    "first_weights = pd.DataFrame(first_weights.transpose())\n",
    "first_weights.columns = words\n",
    "first_weightabs = np.abs(first_weights)\n",
    "first_weightsum = first_weightabs.sum(axis=0)\n",
    "content = np.abs(first_weightsum).to_dict()\n",
    "wordcloud = WordCloud(background_color='black',\n",
    "                      width=1920,\n",
    "                      height=1080\n",
    "                      ).generate_from_frequencies(content)\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/weights_wordcloud.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Vergleichsmodell\n",
    "Training einer RF auf dem bow Input und Evaluierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, max_depth=10,random_state=0,criterion='entropy')\n",
    "RF.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bool_RF = RF.predict(X_test)\n",
    "y_pred_RF = RF.predict_proba(X_test)\n",
    "y_pred_RF = y_pred_RF[:,1]\n",
    "y_pred_train_RF = RF.predict_proba(X_train)\n",
    "y_pred_train_RF = y_pred_train_RF[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.76      0.81      3650\n",
      "         1.0       0.83      0.92      0.87      4714\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      8364\n",
      "   macro avg       0.85      0.84      0.84      8364\n",
      "weighted avg       0.85      0.85      0.85      8364\n",
      "\n",
      "[[2762  888]\n",
      " [ 380 4334]]\n",
      "Predicted   0.0   1.0\n",
      "Actual               \n",
      "0.0        2762   888\n",
      "1.0         380  4334\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, y_pred_bool_RF))\n",
    "print(confusion_matrix(Y_test, y_pred_bool_RF,labels=[0,1]))\n",
    "\n",
    "#Confusion Matrix\n",
    "cnfn_matrix = pd.crosstab(Y_test, y_pred_bool_RF, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(cnfn_matrix)\n",
    "cnfn_matrix.columns = ['fake','real']\n",
    "cnfn_matrix = cnfn_matrix.rename_axis(\"Predicted\", axis=\"columns\")\n",
    "cnfn_matrix.rename(index = {0.0: \"fake\", 1.0:'real'}, inplace = True) \n",
    "cnfn_matrix = cnfn_matrix/Y_test.shape[0]\n",
    "sn.heatmap(cnfn_matrix, annot=True , cmap='viridis')\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/RF/cnfsn_mtx_bow_best_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "#Overtraining test\n",
    "bin_edges = np.linspace(0,1,11)\n",
    "plt.hist(y_pred_RF[Y_test == 0],label=\"fake test\", alpha = 0.4, color = \"r\",density=True,bins=bin_edges)\n",
    "plt.hist(y_pred_train_RF[Y_train == 0],label='fake train', alpha = 0.4, color = 'r', histtype='step',density=True,bins=bin_edges)\n",
    "plt.hist(y_pred_RF[Y_test == 1],label = \"real test\",alpha = 0.4, color = \"b\",density=True,bins=bin_edges)\n",
    "plt.hist(y_pred_train_RF[Y_train == 1],label='real train', alpha = 0.4, color = 'b', histtype='step',density=True,bins=bin_edges)\n",
    "\n",
    "plt.xlabel(\"Prediction Probability\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.legend(loc='upper center')\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/RF/prob_bow_best_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "fpr_RF = dict()\n",
    "tpr_RF = dict()\n",
    "roc_auc_RF = dict()\n",
    "fpr_RF, tpr_RF, _ = roc_curve(Y_test, y_pred_RF)\n",
    "roc_auc_RF = auc(fpr_RF, tpr_RF)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_RF, tpr_RF, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc_RF)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/RF/roc_Hyperopt_bow_best_nn.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Vergleich des Sequential mit dem RF in der ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='Sequential (area = %0.2f)' % roc_auc)\n",
    "plt.plot(fpr_RF, tpr_RF, color='darkred',\n",
    "         lw=lw, label='RandomForest (area = %0.2f)' % roc_auc_RF)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/roc_comparison.pdf\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
