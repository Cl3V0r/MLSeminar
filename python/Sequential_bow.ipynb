{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimierung und Evaluierung des Sequential bow Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import h5py\n",
    "import pydot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, normal, qlognormal, randint\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Activation\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_bow():\n",
    "    x_test = pd.read_hdf(\"../build/preprocessed/bow_data_500.hdf5\",key=\"test\")\n",
    "    x_train = pd.read_hdf(\"../build/preprocessed/bow_data_500.hdf5\",key=\"train\")\n",
    "    y_test = x_test.label\n",
    "    y_train = x_train.label\n",
    "    x_test= x_test.drop('label',axis=1)\n",
    "    x_train = x_train.drop('label',axis=1)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_hdf(\"../build/preprocessed/bow_data_500.hdf5\",key=\"test\")\n",
    "x_train = pd.read_hdf(\"../build/preprocessed/bow_data_500.hdf5\",key=\"train\")\n",
    "y_test = x_test.label\n",
    "y_train = x_train.label\n",
    "x_test= x_test.drop('label',axis=1)\n",
    "x_train = x_train.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = x_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(1770, kernel_regularizer=l1(2.0036577552673407e-06), input_dim=dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(9,kernel_regularizer=l2(0.05407632514834404)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "\n",
    "            optimizer='Adagrad')\n",
    "\n",
    "model.summary()\n",
    "    \n",
    "result = model.fit(x_train.values, y_train.values,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochen')\n",
    "    plt.ylabel('Verlust')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validierung'])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "plot_history(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_hdf(\"../build/preprocessed/bow_data_500.hdf5\",key=\"test\")\n",
    "x_train = pd.read_hdf(\"../build/preprocessed/bow_data_500.hdf5\",key=\"train\")\n",
    "X = x_test.append(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$max\\left(\\frac{(\\bar{w_1}-\\bar{w_2})^2}{s_1^2+s_2^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_real = X[X.label==1].mean()\n",
    "mu_fake = X[X.label==0].mean()\n",
    "sorted_words = ((mu_real-mu_fake)**2/(np.var(X[X.label==1])**2 + np.var(X[X.label==0])**2)).sort_values(ascending=False).index\n",
    "\n",
    "words_plot = pd.DataFrame({'fake':X[X.label==0][sorted_words[1:11]].mean(),'real':X[X.label==1][sorted_words[1:11]].mean()})\n",
    "words_plot.plot(kind='bar')\n",
    "plt.ylabel(r\"$\\overline{w}$\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"../build/plots/data_visualisation.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('../data/mixed_news/news_dataset.csv')\n",
    "news = news.dropna(subset=['title','content'])\n",
    "news = news[news.content != ' ']\n",
    "news = news[news.title != ' ']\n",
    "text_len_real  =  [len(c) for c in news[news['label']=='real'].content]\n",
    "text_len_fake = [len(c) for c in news[news['label']=='fake'].content]\n",
    "print(\"Mittlere textl√§nge Real: \",np.mean(text_len_real))\n",
    "print(\"Mittlere Textl√§nge Fake: \",np.mean(text_len_fake))\n",
    "print(\"Real ist %d l√§nger wie Fake: \" % (np.mean(text_len_real)-np.mean(text_len_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How could the Hyperparameter be distributed\n",
    "\n",
    "### Strukture\n",
    "\n",
    "Gr√∂√üe der ersten und zweiten hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.round(np.random.lognormal(6,0.5,10000)/10)*10\n",
    "plt.hist(x,bins=100)\n",
    "plt.xlim(0,5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gr√∂√üer der dritten hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.round(np.random.lognormal(4,0.5,10000)/1)*1\n",
    "plt.hist(x,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =np.random.uniform(0,0.1,10000)\n",
    "plt.hist(x,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_structure(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int({{qlognormal(6,0.5,10)}}), input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    if {{choice(['three', 'four'])}} == 'four':\n",
    "        model.add(Dense(int({{qlognormal(6,0.5,10)}})))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "    model.add(Dense(int({{qlognormal(4,0.5,1)}})))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer='adam')\n",
    "\n",
    "    result = model.fit(x_train.values, y_train.values,\n",
    "              batch_size=64,\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_split=0.3)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1770, input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(9))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss={{choice(['hinge','binary_crossentropy','squared_hinge'])}}, metrics=['accuracy'],\n",
    "                  optimizer={{choice(['adam','AdaDelta','Adagrad'])}})\n",
    "\n",
    "    result = model.fit(x_train.values, y_train.values,\n",
    "              batch_size=64,\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_split=0.3)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_regularization(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1770, kernel_regularizer=l1({{uniform(0,0.1)}}), input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(9,kernel_regularizer=l2({{uniform(0,0.1)}})))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer='Adagrad')\n",
    "\n",
    "    result = model.fit(x_train.values, y_train.values,\n",
    "                      batch_size=64,\n",
    "                      epochs=30,\n",
    "                      verbose=2,\n",
    "                      validation_split=0.3)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_optimizer(x_train, y_train, x_test, y_test):\n",
    "    dim = x_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1770, kernel_regularizer=l1(2.0*10**(-6)), input_dim=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(9,kernel_regularizer=l2(0.05407632514834404)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer=Adagrad(lr={{uniform(0,1)}}, epsilon=None, decay={{uniform(0,1)}}))\n",
    "\n",
    "    result = model.fit(x_train.values, y_train.values,\n",
    "                      batch_size=64,\n",
    "                      epochs=30,\n",
    "                      verbose=2,\n",
    "                      validation_split=0.3)\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with hyperopt\n",
    "Algorithm: Tree of Parzen Estimators\n",
    "Optimierung in 3 Schritten:\n",
    "    - Struktur (Tiefe (2 oder 3 hidden Layers) und Breite)\n",
    "    - Training (loss function und optimizer)\n",
    "    - Regularizierung ( L1 f√ºr die erste Layer und L2 f√ºr 2 und 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model_structure,\n",
    "                                      data=data_bow,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=trials,\n",
    "                                     notebook_name='Sequential_bow')\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.save('../model/best_Hyperopt_NN_bow_struct_500.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model_training,\n",
    "                                      data=data_bow,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=15,\n",
    "                                      trials=trials,\n",
    "                                     notebook_name='Sequential_bow')\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.save('../model/best_Hyperopt_NN_bow_training_500.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model_regularization,\n",
    "                                      data=data_bow,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=80,\n",
    "                                      trials=trials,\n",
    "                                     notebook_name='Sequential_bow')\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.save('../model/best_Hyperopt_NN_bow_regularization2_500.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model_optimizer,\n",
    "                                      data=data_bow,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=100,\n",
    "                                      trials=trials,\n",
    "                                     notebook_name='Sequential_bow')\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.save('../model/best_Hyperopt_NN_bow_optimizer_500.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beste Regularization: \n",
    "\n",
    "L1 in der ersten Layer = 0.00010911483516010123\n",
    "\n",
    "L2 in der zweiten Layer = 0.027248712155710758\n",
    "\n",
    "lr = 0.12056175158012145\n",
    "\n",
    "epsilon = 0.1999709211230266\n",
    "\n",
    "Die Optimierung des Lernrate f√ºhrt jedoch zu einem Optimizer, der leicht in Nebenmaxima stecken bleibt (hier das Nebenmaxima, dass alles auf real zu sch√§tzen). Daher wird der Standard Lerner verwendet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train best model\n",
    "Neues Training des besten Modells, welches Optimiert bez√ºglich der Hyperparameter ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochen')\n",
    "    plt.ylabel('Verlust')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validierung'])\n",
    "    #plt.show()\n",
    "    plt.savefig(\"../build/plots/bow/500/history_bow_best.pdf\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "dot = Digraph(comment='The Round Table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/larsmoellerherm/.local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/larsmoellerherm/.local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_124 (Dense)            (None, 1770)              886770    \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 1770)              0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 9)                 15939     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 902,719\n",
      "Trainable params: 902,719\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_42',\n",
       " 'layers': [{'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_124',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 500),\n",
       "    'dtype': 'float32',\n",
       "    'units': 1770,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': {'class_name': 'L1L2',\n",
       "     'config': {'l1': 0.00010911483695963398, 'l2': 0.0}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_124',\n",
       "    'trainable': True,\n",
       "    'activation': 'relu'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_125',\n",
       "    'trainable': True,\n",
       "    'units': 9,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': {'class_name': 'L1L2',\n",
       "     'config': {'l1': 0.0, 'l2': 0.02724871225655079}},\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_125',\n",
       "    'trainable': True,\n",
       "    'activation': 'relu'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_126',\n",
       "    'trainable': True,\n",
       "    'units': 1,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Activation',\n",
       "   'config': {'name': 'activation_126',\n",
       "    'trainable': True,\n",
       "    'activation': 'sigmoid'}}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data_bow()\n",
    "best_model = load_model('../model/best_Hyperopt_NN_bow_regularization2_500.hdf5')\n",
    "model = Sequential.from_config(best_model.get_config())\n",
    "print(model.summary())\n",
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../model/best_Hyperopt_NN_bow_trained_500.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "#stopping = EarlyStopping(monitor='val_loss', min_delta=0, restore_best_weights=True)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adagrad',#(lr=0.12056175158012145, epsilon=0.1999709211230266),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train.values, Y_train.values, validation_split=0.3,\n",
    "                    epochs=100,batch_size=64, callbacks=[checkpoint])#, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best model\n",
    "Betrachten des trainierten Modells. Darstellung der Confusion Matrix, Overtraining Plot und ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8371/8371 [==============================] - 1s 173us/step\n",
      "19532/19532 [==============================] - 2s 114us/step\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model('../model/best_Hyperopt_NN_bow_trained_500.hdf5')\n",
    "y_pred = best_model.predict(X_test.values, batch_size=64, verbose=1)\n",
    "y_pred_train = best_model.predict(X_train.values, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.round(y_pred[:,0])\n",
    "Y_test = pd.DataFrame({\"label\":Y_test,\"prediction\":y_pred[:,0],\"prediction_bool\":y_pred_bool})\n",
    "Y_train = pd.DataFrame({\"label\":Y_train,\"prediction\":y_pred_train[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      3658\n",
      "           1       0.89      0.90      0.90      4713\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      8371\n",
      "   macro avg       0.88      0.88      0.88      8371\n",
      "weighted avg       0.88      0.88      0.88      8371\n",
      "\n",
      "Binary Cross Entropie:  0.31712685092066817\n",
      "Predicted   0.0   1.0\n",
      "Actual               \n",
      "0          3150   508\n",
      "1           481  4232\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test['label'], Y_test['prediction_bool']))\n",
    "print(\"Binary Cross Entropie: \",log_loss(Y_test.label, Y_test.prediction))\n",
    "\n",
    "#Confusion Matrix\n",
    "cnfn_matrix = pd.crosstab(Y_test['label'], Y_test['prediction_bool'], rownames=['Wahrheit'], colnames=['Sch√§tzung'])\n",
    "print(cnfn_matrix)\n",
    "cnfn_matrix.columns = ['Fake','Real']\n",
    "cnfn_matrix = cnfn_matrix.rename_axis(\"Sch√§tzung\", axis=\"columns\")\n",
    "cnfn_matrix.rename(index = {0.0: \"Fake\", 1.0:'Real'}, inplace = True) \n",
    "cnfn_matrix = cnfn_matrix/Y_test.shape[0]\n",
    "sn.heatmap(cnfn_matrix, annot=True , cmap='viridis')\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/cnfsn_mtx_bow_best_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "#Overtraining test\n",
    "plt.hist(Y_test.prediction[Y_test.label == 0],label=\"fake test\", alpha = 0.4, color = \"r\",density=True)\n",
    "plt.hist(Y_train.prediction[Y_train.label == 0],label='fake train', alpha = 0.4, color = 'r', histtype='step',density=True)\n",
    "plt.hist(Y_test.prediction[Y_test.label == 1],label = \"real test\",alpha = 0.4, color = \"b\",density=True)\n",
    "plt.hist(Y_train.prediction[Y_train.label == 1],label='real train', alpha = 0.4, color = 'b', histtype='step',density=True)\n",
    "\n",
    "plt.xlabel(\"Gesch√§tzte Likelihood\")\n",
    "plt.ylabel(\"Dichte\")\n",
    "plt.legend(loc='upper center')\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/prob_bow_best_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "fpr, tpr, _ = roc_curve(Y_test.label, Y_test.prediction)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC Kurve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/roc_Hyperopt_bow_best_nn.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### Wordcloud confusion matrix\n",
    "\n",
    "Darstellung der Wordh√§ufigkeiten in WordClouds f√ºr FP,FN,TP,TN getrennt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FP = Y_test[(Y_test.prediction_bool== 1) & (Y_test.label == 0)]\n",
    "FN = Y_test[(Y_test.prediction_bool== 0) & (Y_test.label == 1)]\n",
    "TP = Y_test[(Y_test.prediction_bool== 1) & (Y_test.label == 1)]\n",
    "TN = Y_test[(Y_test.prediction_bool== 0) & (Y_test.label == 0)]\n",
    "X_FP = X_test.loc[FP.index]\n",
    "X_FN = X_test.loc[FN.index]\n",
    "X_TP = X_test.loc[TP.index]\n",
    "X_TN = X_test.loc[TN.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWordcloud_cnfn(TN,FN,FP,TP):    \n",
    "    TN = TN.sum().to_dict()\n",
    "    FN = FN.sum().to_dict()\n",
    "    FP = FP.sum().to_dict()\n",
    "    TP = TP.sum().to_dict()\n",
    "    \n",
    "    pad = 5\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,10),dpi=100)\n",
    "\n",
    "    ax = plt.subplot(2, 2, 1)\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                          width=1920,\n",
    "                          height=1080,\n",
    "                          mask=np.array(Image.open('../data/pictures/trump_silhouette.png'))\n",
    "                          ).generate_from_frequencies(TN)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    ax = plt.subplot(2, 2, 2)\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                          width=1920,\n",
    "                          height=1080,\n",
    "                          mask= np.array(Image.open('../data/pictures/trump_silhouette.png'))\n",
    "                          ).generate_from_frequencies(FP)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    ax = plt.subplot(2, 2, 3)\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                          width=1920,\n",
    "                          height=1080,\n",
    "                          mask=np.array(Image.open('../data/pictures/USA.jpg'))\n",
    "                          ).generate_from_frequencies(FN)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                          width=1920,\n",
    "                          height=1080,\n",
    "                          mask=np.array(Image.open('../data/pictures/USA.jpg'))\n",
    "                          ).generate_from_frequencies(TP)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.figtext(0.5, 1.09, r\"Prediction\", {'fontsize': 30},\n",
    "         horizontalalignment='center',\n",
    "         verticalalignment='top')\n",
    "    plt.figtext(0.25, 1.02, r\"fake\", {'fontsize': 20},\n",
    "         horizontalalignment='center',\n",
    "         verticalalignment='bottom',)\n",
    "    plt.figtext(0.75, 1.02, r\"real\", {'fontsize': 20},\n",
    "         horizontalalignment='center',\n",
    "         verticalalignment='bottom',)\n",
    "    \n",
    "    plt.figtext(-0.07, 0.5, r\"Actual\", {'fontsize': 30},\n",
    "         horizontalalignment='left',\n",
    "         verticalalignment='center',\n",
    "         rotation=90)\n",
    "    plt.figtext(0.00, 0.75, r\"fake\", {'fontsize': 20},\n",
    "         horizontalalignment='right',\n",
    "         verticalalignment='center',)\n",
    "    plt.figtext(0.00, 0.25, r\"real\", {'fontsize': 20},\n",
    "         horizontalalignment='right',\n",
    "         verticalalignment='center',)\n",
    "    \n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(\"../build/plots/bow/500/cnfn_wordcloud.pdf\", bbox_inches = 'tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotWordcloud_cnfn(X_TN,X_FN,X_FP,X_TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud fake real news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWordcloud(content,t):\n",
    "    if(t!=\"\"):\n",
    "       mask = np.array(Image.open('../data/pictures/'+t))\n",
    "    else:\n",
    "        mask=None\n",
    "        \n",
    "\n",
    "    content = content.sum().to_dict()\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                      width=1920,\n",
    "                      height=1080,\n",
    "                      mask=mask\n",
    "                      ).generate_from_frequencies(content)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test.append(X_train)\n",
    "y = Y_test.append(Y_train)\n",
    "plt.figure(dpi=200)\n",
    "plotWordcloud(X[y.label==0],\"trump_silhouette.png\")\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/fake_wordcloud.pdf\",bbox_inches='tight',pad_inches = 0)\n",
    "plt.close()\n",
    "plt.figure(dpi=200)\n",
    "plotWordcloud(X[y.label==1],\"USA.jpg\")\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/real_wordcloud.pdf\",bbox_inches='tight',pad_inches = 0)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untersuchung der first layer\n",
    "\n",
    "Summieren der Betr√§ge aller Gewichte eines Neurons ohne Offset und anschlie√üende Darstellung in WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1770)\n"
     ]
    }
   ],
   "source": [
    "words = X_test.columns\n",
    "first_weights = best_model.layers[0].get_weights()[0]\n",
    "first_weights = pd.DataFrame(first_weights.transpose())\n",
    "first_weights.columns = words\n",
    "first_weightabs = np.abs(first_weights)\n",
    "first_weightsum = first_weightabs.sum(axis=0)\n",
    "content = np.abs(first_weightsum).to_dict()\n",
    "wordcloud = WordCloud(background_color='black',\n",
    "                      width=1920,\n",
    "                      height=1080\n",
    "                      ).generate_from_frequencies(content)\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/weights_wordcloud.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untersuchung der Confusion Matrix mithilfe der wichtigsten 10 W√∂rter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_weights = first_weightsum.sort_values(ascending=False)\n",
    "best_words = sorted_weights[:10].index\n",
    "\n",
    "fig = plt.figure(figsize=(15,10),dpi=100)\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "(X_TN[best_words].sum()/X_TN.shape[0]).plot(kind='bar',label=\"TN\",color='r')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.ylabel(\"mittlere Worth√§ufigkeit\")\n",
    "plt.ylim(0.01,60)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "(X_FP[best_words].sum()/X_FP.shape[0]).plot(kind='bar',label=\"FP\",color='g')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.ylabel(\"mittlere Worth√§ufigkeit\")\n",
    "plt.ylim(0.01,60)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "(X_FN[best_words].sum()/X_FN.shape[0]).plot(kind='bar',label=\"FN\",color='k')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.ylabel(\"mittlere Worth√§ufigkeit\")\n",
    "plt.ylim(0.01,60)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "(X_TP[best_words].sum()/X_TP.shape[0]).plot(kind='bar',label=\"TP\",color='b')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.ylabel(\"mittlere Worth√§ufigkeit\")\n",
    "plt.ylim(0.01,60)\n",
    "plt.yscale(\"log\")\n",
    "    \n",
    "plt.figtext(0.5, 1.09, r\"Sch√§tzung\", {'fontsize': 30},\n",
    "     horizontalalignment='center',\n",
    "     verticalalignment='top')\n",
    "plt.figtext(0.25, 1.02, r\"Fake\", {'fontsize': 20},\n",
    "     horizontalalignment='center',\n",
    "     verticalalignment='bottom',)\n",
    "plt.figtext(0.75, 1.02, r\"Real\", {'fontsize': 20},\n",
    "     horizontalalignment='center',\n",
    "     verticalalignment='bottom',)\n",
    "    \n",
    "plt.figtext(-0.07, 0.5, r\"Wahrheit\", {'fontsize': 30},\n",
    "     horizontalalignment='left',\n",
    "     verticalalignment='center',\n",
    "     rotation=90)\n",
    "plt.figtext(0.00, 0.75, r\"Fake\", {'fontsize': 20},\n",
    "     horizontalalignment='right',\n",
    "     verticalalignment='center',)\n",
    "plt.figtext(0.00, 0.25, r\"Real\", {'fontsize': 20},\n",
    "     horizontalalignment='right',\n",
    "     verticalalignment='center',)\n",
    "    \n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/cnfn_hist.pdf\", bbox_inches = 'tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Vergleichsmodell\n",
    "Training einer RF auf dem bow Input und Evaluierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=100, max_depth=10,random_state=0,criterion='entropy')\n",
    "RF.fit(X_train.values,Y_train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bool_RF = RF.predict(X_test.values)\n",
    "y_pred_RF = RF.predict_proba(X_test.values)\n",
    "y_pred_RF = y_pred_RF[:,1]\n",
    "y_pred_train_RF = RF.predict_proba(X_train.values)\n",
    "y_pred_train_RF = y_pred_train_RF[:,1]\n",
    "Y_test['prediction_RF'] = y_pred_RF\n",
    "Y_test['prediction_bool_RF'] = y_pred_bool_RF\n",
    "Y_train['prediction_RF'] = y_pred_train_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test.label, Y_test.prediction_bool_RF))\n",
    "\n",
    "#Confusion Matrix\n",
    "cnfn_matrix = pd.crosstab(Y_test.label, Y_test.prediction_bool_RF, rownames=['Wahrheit'], colnames=['Sch√§tzung'])\n",
    "print(cnfn_matrix)\n",
    "cnfn_matrix.columns = ['Fake','Real']\n",
    "cnfn_matrix = cnfn_matrix.rename_axis(\"Sch√§tzung\", axis=\"columns\")\n",
    "cnfn_matrix.rename(index = {0.0: \"Fake\", 1.0:'Real'}, inplace = True) \n",
    "cnfn_matrix = cnfn_matrix/Y_test.shape[0]\n",
    "sn.heatmap(cnfn_matrix, annot=True , cmap='viridis')\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/RF/cnfsn_mtx_bow_best_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "#Overtraining test\n",
    "bin_edges = np.linspace(0,1,11)\n",
    "plt.hist(Y_test.prediction_RF[Y_test.label == 0],label=\"fake test\", alpha = 0.4, color = \"r\",density=True,bins=bin_edges)\n",
    "plt.hist(Y_train.prediction_RF[Y_train.label == 0],label='fake train', alpha = 0.4, color = 'r', histtype='step',density=True,bins=bin_edges)\n",
    "plt.hist(Y_test.prediction_RF[Y_test.label == 1],label = \"real test\",alpha = 0.4, color = \"b\",density=True,bins=bin_edges)\n",
    "plt.hist(Y_train.prediction_RF[Y_train.label == 1],label='real train', alpha = 0.4, color = 'b', histtype='step',density=True,bins=bin_edges)\n",
    "\n",
    "plt.xlabel(\"Gesch√§tzte Likelihood\")\n",
    "plt.ylabel(\"Dichte\")\n",
    "plt.legend(loc='upper center')\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/RF/prob_bow_best_nn.pdf\")\n",
    "plt.close()\n",
    "\n",
    "fpr_RF = dict()\n",
    "tpr_RF = dict()\n",
    "roc_auc_RF = dict()\n",
    "fpr_RF, tpr_RF, _ = roc_curve(Y_test.label, Y_test.prediction_RF)\n",
    "roc_auc_RF = auc(fpr_RF, tpr_RF)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_RF, tpr_RF, color='darkorange',\n",
    "         lw=lw, label='ROC Kurve (AUC = %0.2f)' % roc_auc_RF)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/RF/roc_Hyperopt_bow_best_nn.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Vergleich des Sequential mit dem RF in der ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='Sequential (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot(fpr_RF, tpr_RF, color='darkred',\n",
    "         lw=lw, label='RandomForest (AUC = %0.2f)' % roc_auc_RF)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "plt.savefig(\"../build/plots/bow/500/roc_comparison.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../build/plots/bow/500/modell_scheme.pdf'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "m = Digraph(name='Modell', node_attr={'shape': 'record'})\n",
    "m.attr(rankdir='LR')\n",
    "\n",
    "m.node(\"input\", r\"ùêÑùê¢ùêßùê†ùêöùêßùê†ùê¨ ùêãùêöùê†ùêû | 500 W√∂rter \")\n",
    "m.node(\"first\", r\"ùêÑùê´ùê¨ùê≠ùêû ùêãùêöùê†ùêû|Neuronen: 1770|Aktivierung: ReLu|L1: 0.00011\")\n",
    "m.node(\"second\", r\"ùêôùê∞ùêûùê¢ùê≠ùêû ùêãùêöùê†ùêû|Neuronen: 9|Aktivierung: ReLu|L2: 0.0272\")\n",
    "m.node(\"output\", r\"ùêÄùêÆùê¨ùê†ùêöùêßùê†ùê¨ ùêãùêöùê†ùêû|Neuronen: 1 |Aktivierung: Sigmoid\")\n",
    "\n",
    "m.edge('input', 'first')\n",
    "m.edge('first', 'second')\n",
    "m.edge('second', 'output')\n",
    "\n",
    "m.render('../build/plots/bow/500/modell_scheme', view=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
